{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Wine Price Prediction - Model Definition v3\n",
    "\n",
    "- **v1** - Trained simple DL model and LGB model\n",
    "- **v2** - Added wider and deeper DL models\n",
    "- **v3** - Update model configurations for updated dataset\n",
    "\n",
    "## Data and Use case\n",
    "\n",
    "[**Wine Reviews** - 130k wine reviews with variety, location, winery, price, and description](https://www.kaggle.com/zynicide/wine-reviews/home)\n",
    "\n",
    "This dataset is available on Kaggle and contains around 130k of wine reviews. The data was scraped from [WineEnthusiast](http://www.winemag.com/?s=&drink_type=wine) on November 22nd, 2017.\n",
    "\n",
    "I plan to use this dataset to develop a model that predicts wine price for specified set of parameters, like wine variety, region, desired quality. Such model, may be integrated into an application that runs on a mobile device to suggest price range during wine shopping without need to do online search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our problem is a supervised machine learning task: basing on historical data we want to predict wine price values. As we saw, our dataset contains mostly categorical features (at the first iteration at least). \n",
    "\n",
    "According to the course requirements we need to implement our algorithm in at least one deep learning and at least one non-deep learning algorithm. Let's start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with choosing a model performance indicator first.  \n",
    "\n",
    "For our problem we select **the mean squared error (MSE)** that measures the average of the squares of the errors â€” that is, the average squared difference between the estimated values and what is estimated. MSE has the same units of measurement as the square of the quantity being estimated.\n",
    "\n",
    "I will also look at the square root of MSE that yields **the root-mean-square error (RMSE)**, which has the same units as the quantity being estimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to create an MLP model using Keras to solve this regression problem. Keras allows to define MLP model in a quite simple and quick way. It's also supported in Watson Studio with TensorFlow as a back-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with relatively simple MLP network that includes just two layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_features = 1146 # number of inputs\n",
    "\n",
    "model = Sequential()  # Instantiate sequential model\n",
    "model.add(Dense(1200, activation='relu', input_shape=(num_of_features,)))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile our model with Adam optimizer and MSE as a loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our model for future training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"wine-price-prediction.model.keras.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()  # Instantiate sequential model\n",
    "model.add(Dense(1536, activation='relu', input_shape=(num_of_features,)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model.save(\"wine-price-prediction.model.keras.deep.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wider Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()  # Instantiate sequential model\n",
    "model.add(Dense(3000, activation='relu', input_shape=(num_of_features,)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "model.save(\"wine-price-prediction.model.keras.wide.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Deep Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our problem is a supervised machine learning task: basing on historical data we want to predict wine price values. As we saw, our dataset contains mostly categorical features (at the first iteration at least). Tree models are particularly well suited for this type of dataset since each tree branching can be mapped to a distinct value from a categorical datum.\n",
    "\n",
    "Tree models are often so effective at fitting the data that they usually do suffer from overfitting. A common solution is to consider tree ensembles to increase variance without reducing bias to much.\n",
    "\n",
    "Here we have chosen a Gradient Boosted Trees model were an ensemble of trees are successively trained in a sequence, where a new tree is trained on a subset of date that was not correctly predicted by its predecessor, adding to the collective prediction of the tree ensemble. Trees can grow exponentially large for large datasets with many variables. In our case the data set is relatively small.\n",
    "\n",
    "I am going to use LightGBM gradient boosting framework (https://lightgbm.readthedocs.io/en/latest/). This framework seems to be quite popular on Kaggle and provides Python API, so I will be able to use it in Watson Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMModel\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LGBMModel(objective=\"regression\", metric=[\"mse\", \"rmse\"], num_leaves=50, \n",
    "                   learning_rate=0.01, bagging_fraction=0.75, feature_fraction=0.8, \n",
    "                   bagging_frequency=9, n_estimators=5000, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wine-price-prediction.model.lgbm.pickle', 'wb') as file:\n",
    "    pickle.dump(model2, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
